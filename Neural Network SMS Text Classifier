import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np

# Assume train_data and test_data are pandas DataFrames with 'text' and 'label' columns

# Convert labels to 0 (ham) and 1 (spam)
label_map = {'ham': 0, 'spam': 1}
train_labels = train_data['label'].map(label_map).values
test_labels = test_data['label'].map(label_map).values

# Tokenize text
tokenizer = Tokenizer(num_words=10000, oov_token="<OOV>")
tokenizer.fit_on_texts(train_data['text'])

# Convert texts to sequences
train_sequences = tokenizer.texts_to_sequences(train_data['text'])
test_sequences = tokenizer.texts_to_sequences(test_data['text'])

# Pad sequences to same length
max_length = 100
train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post', truncating='post')
test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post', truncating='post')
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(10000, 16, input_length=max_length),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(16, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')  # output probability of spam
])

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

model.summary()
EPOCHS = 10
history = model.fit(
    train_padded, train_labels,
    epochs=EPOCHS,
    validation_data=(test_padded, test_labels),
    verbose=2
)
def predict_message(message):
    seq = tokenizer.texts_to_sequences([message])
    padded = pad_sequences(seq, maxlen=max_length, padding='post', truncating='post')
    prob = model.predict(padded)[0][0]  # probability spam
    
    label = 'spam' if prob > 0.5 else 'ham'
    return [prob, label]
